{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_Ngy5iyrXROK",
        "Z4CcXsP8xvPz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages"
      ],
      "metadata": {
        "id": "CBCBIXu2MYWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Local System\n",
        "If you want to run this Jupyter notebook on your local system, you only need to install *PySpark*. PySpark installs Spark under the hood. However, be aware that on your local system, you have only one node in your cluster, so you cannot leverage the full power of Spark. It is also recommended to install *FindSpark*, which helps locate Spark in the system.\n",
        "\n",
        "**âš ** Another thing to note when running a Spark application on a single node is *closures* and *variable sharing*. Your application might work on your local system but not perform as expected in a multi-node cluster.\n",
        "\n"
      ],
      "metadata": {
        "id": "2SDE6SDHVq1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae7OO6gGLv7j",
        "outputId": "c4fd88c0-6044-4410-bbc5-77a8da082a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=5b18d8b7740ad369b8f14d6c53a30f1c91d4c047208aa56358972aac456056dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, pyspark\n",
            "Successfully installed findspark-2.0.1 pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Google Colab\n",
        "If you want to run this Jupyter notebook on Google Colab, you need to execute the following cells.\n",
        "\n",
        "The following code will install the necessary applications and packages for working with Spark. These packages are:\n",
        "- JDK: Java Development Kit\n",
        "- Spark 3.5.1 (Released February 2024)\n",
        "- Findspark (used to locate Spark in the system)\n"
      ],
      "metadata": {
        "id": "_Ngy5iyrXROK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Xss1AsHCVyfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setting environmental variables"
      ],
      "metadata": {
        "id": "bINrDj6Cazyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "zivTH4nUanuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zabN6720bCPx",
        "outputId": "57fc0bc6-15f6-462e-8153-9b30551983b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.5.1-bin-hadoop3  spark-3.5.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verifying the Installation of PySpark\n"
      ],
      "metadata": {
        "id": "uaWUkIA6Mg_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xfsfEuJ6b3BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Expolring_spark').getOrCreate()\n",
        "\n",
        "print('Spark version: ', spark.version)\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2vKT-jlLNq5",
        "outputId": "80ca1533-d406-44f8-fa64-dfa21187eb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version:  3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Synthetic Data for Our Analysis\n",
        "To start, we will generate some synthetic data to work with. Let's assume we have a dataset that reflects customer behavior in an online shop. This dataset will contain one million records and will be saved in a text file, with each line representing one record. The fields in our dataset include:\n",
        "\n",
        "- **User ID**: A unique identifier for each user.\n",
        "- **Timestamp**: The date and time of the interaction.\n",
        "- **Interaction Type**: The type of interaction the user had  with the shop, such as \"*click*,\" \"*view*,\" or \"*purchase*.\"\n",
        "- **Item ID**: A unique identifier for each item in the shop.\n",
        "- **Item Category**: The category of the item, such as \"*appliance*\", \"*electronics*,\" \"*books*,\" \"*clothing*,\" or \"*food*.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4CcXsP8xvPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "random.seed(126)#for reproducibility\n",
        "def generate_data(num_records):\n",
        "  '''\n",
        "  Generates syntetic data.\n",
        "  Parameters:\n",
        "  ------------\n",
        "  num_records: the number of generated records\n",
        "\n",
        "  smaples:\n",
        "  --------\n",
        "  user_id, timestamp, interaction, item_id, item_category\n",
        "  9432, 2024-07-29T12:22:32.732595, view, 4619, books\n",
        "  2097, 2024-07-23T04:04:25.732595, view, 3673, clothing\n",
        "  '''\n",
        "  start_date = datetime.now()\n",
        "  for _ in range(num_records):\n",
        "      user_id = random.randint(1, 10_000)\n",
        "      timestamp = (start_date - timedelta(seconds=random.randint(0, 2_592_000))).isoformat()\n",
        "      interaction = random.choice([\"click\", \"view\", \"purchase\"])\n",
        "      item_id = random.randint(1, 5_000)\n",
        "      item_category = random.choice([\"appliance\", \"electronics\", \"books\", \"clothing\", \"food\" ])\n",
        "      yield \", \".join(map(str, [user_id, timestamp, interaction, item_id, item_category]))\n",
        "\n",
        "def write_data_to_file(file_path, num_records, buffer_size=1000, overwrite_file=True):\n",
        "  '''\n",
        "  Generates syntetic data and Writes to a file.\n",
        "  We are using a buffer to reduce the number of I/O operations in order to increase efficiency.\n",
        "\n",
        "  Parampeters:\n",
        "  ------------\n",
        "  file_path: the output file path\n",
        "  num_records: the number of generated records\n",
        "  buffer_size: the size of buffer data that is used to keep records in memory before writing them to the output file\n",
        "  overwrite_file: a flag that determine weather to overwrite the file or not.\n",
        "  '''\n",
        "  if overwrite_file:\n",
        "    open(file_path, \"w\").close() #overwrite the contet of the file\n",
        "\n",
        "  with open(file_path, \"a\") as f:\n",
        "    buffer = []\n",
        "    for line in tqdm(generate_data(num_records), total=num_records, desc=\"Writing data\"):\n",
        "        buffer.append(line + \"\\n\")\n",
        "        if len(buffer) >= buffer_size:\n",
        "            f.writelines(buffer)\n",
        "            buffer = []\n",
        "\n",
        "    if buffer:  # Write remaining lines in buffer\n",
        "        f.writelines(buffer)"
      ],
      "metadata": {
        "id": "AiH3mJahyZYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write 1,000,000 records to data.txt\n",
        "write_data_to_file(\"data.txt\", 1_000_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jriNxhQrDY9E",
        "outputId": "c2bd1969-a257-4021-8c78-2878062b35c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:10<00:00, 93701.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD\n",
        "RDD stands for **R**esilient **D**istributed **D**ataset and is the fundamental building block of Apache Spark. While higher-level abstractions like `DataFrames` and `Datasets` are built on top of RDDs, understanding RDDs is crucial for mastering Spark.\n",
        "\n",
        "**What is an RDD?**\n",
        "\n",
        "An RDD is a *collection* of elements *partitioned* across the nodes of a cluster that can be operated on in parallel. This parallelism allows Spark to handle large-scale data processing efficiently.\n",
        "\n",
        "**Key Features of RDDs**\n",
        "- **Resilient**: RDDs are fault-tolerant and can  automatically recover from node failures. This resilience is achieved through *Directed Acyclic Graph* (DAG), which tracks the series of transformations that resulted the RDD.\n",
        "- **Distributed**: Data in an RDD is distributed across multiple nodes in the cluster, enabling parallel processing.\n",
        "- **Dataset**: An RDD is essentially a distributed collection of objects.\n",
        "\n",
        "**How RDDs Work**\n",
        "\n",
        "At first, the *driver* (an application) should connect to the Spark cluster. When you put a collection of data into an *RDD*, Spark breaks it into smaller chunks known as *partitions* or slices. These partitions are distributed among nodes across the cluster. When the driver wants to perform a task on the data, Spark copies the task to each node. On each node, workers perform the task on their respective partitions of data. In this manner, Spark uses distributed power to parallelize computation. Obviously, there are many challenges that Spark handles under the hood, making the results appear seamless to the user.\n",
        "\n",
        "When working with RDDs, Spark builds a Directed Acyclic Graph (DAG) of transformations. These transformations (such as `map`, `filter`, and `reduce`) define the computation logic but Spark does not execute them immediately. Spark only triggers the actual computation when an action (like `count`, `collect`, `take`, or `save`) is called by driver. This lazy evaluation optimizes the processing and enables fault tolerance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K0sdQUzVazBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparkContext\n",
        "`SparkContext` is the entry point for connecting to Spark in order to work with RDDs. It is the main gateway through which your Spark application(which is called driver) interacts with the Spark cluster.\n",
        "\n",
        "ðŸ’¡ **Key Points**\n",
        "- A driver can have only one active SparkContext at a time.\n",
        "- Before creating a new SparkContext, you must stop the previous one.\n"
      ],
      "metadata": {
        "id": "kBkgK1pOIdvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Creating a spark context class for woriking with RDD\n",
        "sc = SparkContext(master='local[*]', \\\n",
        "                  appName='Expolring Python Spark RDD and DataFrame basics')\n",
        "\n",
        "#verify everything is working correctly\n",
        "print('Spark version: ', sc.version)\n",
        "\n",
        "print(f\"\\n{'key':^35} | {'value':^20}\")\n",
        "print('-'*35,'|', '-'*20)\n",
        "for i in sc.getConf().getAll():\n",
        "  print(f'{i[0]:<35} | {i[1]:<20}')\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "riggtsAUc4yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498a40ab-4c78-4ae7-ff9b-6e6f372cc04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version:  3.5.1\n",
            "\n",
            "                key                 |        value        \n",
            "----------------------------------- | --------------------\n",
            "spark.app.name                      | Expolring Python Spark RDD and DataFrame basics\n",
            "spark.driver.extraJavaOptions       | -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.executor.id                   | driver              \n",
            "spark.app.id                        | local-1722993256177 \n",
            "spark.app.submitTime                | 1722993230984       \n",
            "spark.app.startTime                 | 1722993256086       \n",
            "spark.driver.port                   | 45531               \n",
            "spark.rdd.compress                  | True                \n",
            "spark.executor.extraJavaOptions     | -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.serializer.objectStreamReset  | 100                 \n",
            "spark.driver.host                   | fd112645ab28        \n",
            "spark.master                        | local[*]            \n",
            "spark.submit.pyFiles                |                     \n",
            "spark.submit.deployMode             | client              \n",
            "spark.ui.showConsoleProgress        | true                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ’¡ In newer versions of Spark, it's recommended to use `SparkSession`, which includes `SparkContext` as an attribute. This approach is **preferred** for applications that use DataFrames or SQL operations."
      ],
      "metadata": {
        "id": "3UApP6uUpnh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Expolring Python Spark RDD and DataFrame basics\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(sc.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-S7Rj8jlaE2",
        "outputId": "1299739c-63b6-4ba1-e260-dbe331fb3454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ’¡ `local` indicates that Spark will run locally on your machine. The asterisk [*] specifies that Spark should use all available CPU cores on the local machine for execution."
      ],
      "metadata": {
        "id": "OyBvGWyhRrjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an RDD\n",
        "There are two ways to create RDDs:\n",
        "- **parallelizing** an existing collection in your driver program\n",
        "- **External resources** such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
        "\n",
        "ðŸ’¡ RDD is **imutable**. After creation it can not be changed."
      ],
      "metadata": {
        "id": "KZ9uIv2VigFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_rdd = sc.parallelize(range(0, 1_000_000), 4)\n",
        "print(f'number of partitions: {range_rdd.getNumPartitions()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6zYoudunt53",
        "outputId": "d44aefe4-ce80-4b80-fc40-670fdbb068b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of partitions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of partitions(slices)**\n",
        "- Data will be broken into partitions and distribute among nodes across cluster.\n",
        "- Normally, Spark tries to set the number of partitions *automatically* based on your cluster. So you don't need to set in manually.\n",
        "- Spark will run **one** task for **each partition**.\n",
        "- Typically you want 2-4 partitions for each CPU in your cluster.\n"
      ],
      "metadata": {
        "id": "BGX0qfgUrE1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â“ Let's see the number of elements in our RDD which we have just created."
      ],
      "metadata": {
        "id": "dVCLIad1tVpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'total number of elemetns = {range_rdd.count():,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDTdpmClrlMf",
        "outputId": "e5bc051f-9542-4a6e-fc10-4085e978d4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of elemetns = 1,000,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark can create distributed datasets from any **storage source** supported by Hadoop:\n",
        " - local file system,\n",
        " - HDFS,\n",
        " - Cassandra,\n",
        " - HBase,\n",
        " - Amazon S3,\n",
        " - etc.\n",
        "\n",
        "\n",
        " Spark supports:\n",
        " - text files,\n",
        " - SequenceFiles (a binary file format used in Hadoop to store key-value pairs),\n",
        " - and any other Hadoop InputFormat."
      ],
      "metadata": {
        "id": "V-yprHk5taIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many elements are stored in each partition(slice)?"
      ],
      "metadata": {
        "id": "FSvb51Q7sPPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd=sc.textFile('data.txt')\n",
        "print(f'number of partitions: {text_rdd.getNumPartitions()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrlgnipwsrA4",
        "outputId": "b1a76134-b976-4210-f9ba-e30229ab8fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of partitions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD Operations\n",
        "RDDs support two types of operations:\n",
        "1.   **transformations**, which *create* a new dataset from an existing one. `map`, `filter`, `flatmap`, `union`, `intersection` are examples of transformations.\n",
        "\n",
        "2.   **actions**, which *return* a value to the driver program after running a computation on the dataset. `collect`, `take`, `count`, `countdistinct` are examples of actions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's do some basic operation and see how fast spark is.\n",
        "\n"
      ],
      "metadata": {
        "id": "j7K2MnTHt6DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Take** and **Collect** are used to return the spark proccessing results to driver program.\n",
        "\n",
        "let's see 5 records of our `range_rdd`"
      ],
      "metadata": {
        "id": "kPNNnz2vwsOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31pQvFtot273",
        "outputId": "f6073b8b-4955-42d9-f3e3-76b5d87f18d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample**: ranodmly select a few elements from RDD."
      ],
      "metadata": {
        "id": "9froMM2rTKpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraction = 7 / range_rdd.count()\n",
        "print(fraction)\n",
        "range_rdd.sample(False, fraction, seed=126).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmL5N4tTTQHf",
        "outputId": "08d1b5b7-ba8d-4c5c-cfab-d63a6d6f6063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[362710, 419395, 619157, 677070, 931036]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map**: Create a new RDD that represent the squared value of each element\n",
        "$$x \\longrightarrow x^2 $$"
      ],
      "metadata": {
        "id": "ayGIB3ha5HgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squared_num_rdd = range_rdd.map(lambda x: x**2)\n",
        "squared_num_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay_n3iHL03nP",
        "outputId": "299a1f93-6b56-4f6a-dd87-fc4d1184583d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 9, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter**: Create a new RDD of numbers whose squared values are odd.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "juySlSQwUsbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "odd_squared_num_rdd = squared_num_rdd.filter(lambda x: x % 2 != 0)\n",
        "odd_squared_num_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsMFsZiWX_d1",
        "outputId": "87520df2-0289-4a93-c311-f118825f6e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 9, 25, 49, 81]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduce**: Compute the sum of the squared numbers. use **cascade** pipline: $$ \\sum{x_i^2} $$"
      ],
      "metadata": {
        "id": "krvsaWrHTTtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "total = range_rdd.map(lambda x: x**2).reduce(lambda x, y: x+y)\n",
        "print(f'sum of squared numbers = {total:,}')\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PFqAe3FTou9",
        "outputId": "b7d630f0-5e0b-4cfa-8cf7-f43c48193c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum of squared numbers = 333,332,833,333,500,000\n",
            "\n",
            "CPU times: user 17.3 ms, sys: 1.14 ms, total: 18.5 ms\n",
            "Wall time: 1.42 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count** count the number of prime number"
      ],
      "metadata": {
        "id": "yZCWn_O5Yy_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def is_prime(n):\n",
        "  if n < 2:\n",
        "    return False\n",
        "  for i in range(2, int(n**0.5) + 1):\n",
        "    if n % i == 0:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "count_prime = range_rdd.filter(is_prime).count()\n",
        "print(f'number of prime numbers = {count_prime:,}')\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2HLbhDgY77t",
        "outputId": "2d852b71-ef2f-44ed-aa39-84db30c7db92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of prime numbers = 78,498\n",
            "\n",
            "CPU times: user 41.5 ms, sys: 5.15 ms, total: 46.7 ms\n",
            "Wall time: 6.19 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map**: Extract the number of digits in each number\n",
        "$$ x âŸ¶ len(x)$$"
      ],
      "metadata": {
        "id": "NucQ13bZ03MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_digit_num_rdd = range_rdd.map(lambda x: (len(str(x)), x))\n",
        "pair_digit_num_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yc2pqU11xEE",
        "outputId": "bba25818-0d63-4120-cb13-c8ee3279bd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GroupByKey**: Groups the numbers based on the number of digits they contain and determines how many numbers belong to each group. For example, the number 9 belongs to group 1, which contains one-digit numbers.\n",
        "\n",
        "ðŸ’¡ Before using `collect`, ensure that your data fits in memory. Do NOT `collect` the entire dataset across the cluster."
      ],
      "metadata": {
        "id": "XQmJkvduxv4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_digit_num_rdd.groupByKey().map(lambda x: (x[0], len(x[1]))).sortByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvXRz48F3iN0",
        "outputId": "65d9ba4a-ee59-4f07-eafe-3f2a4d7d6a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 10), (2, 90), (3, 900), (4, 9000), (5, 90000), (6, 900000)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CountByKey**:We can do the same task with CountByKey action. The output is a Dictionary."
      ],
      "metadata": {
        "id": "CXNus0KZdrZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_digit_num_rdd.countByKey()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqUtHwK9d0iB",
        "outputId": "0f7e0500-bc45-48dd-e03f-0a20cd48e437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {1: 10, 2: 90, 3: 900, 4: 9000, 5: 90000, 6: 900000})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReduceByKey**: There is yet another approach to get the same result"
      ],
      "metadata": {
        "id": "pTZBOuGmDjGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_digit_num_rdd.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y).sortByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAUicvrVDr9z",
        "outputId": "ea358ca2-aac5-4da7-f277-780aed69b0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 10), (2, 90), (3, 900), (4, 9000), (5, 90000), (6, 900000)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mapValues**: Compute the number of elements, minimum, maximum and average for each group."
      ],
      "metadata": {
        "id": "suwir7eI_ygX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_digit_num_rdd.groupByKey().mapValues(lambda x: ( len(x), min(x), max(x) , sum(x)/len(x))).sortByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5QwGYYJ_7lg",
        "outputId": "1cbf2199-ee60-4193-d6c9-b19e5e73e4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, (10, 0, 9, 4.5)),\n",
              " (2, (90, 10, 99, 54.5)),\n",
              " (3, (900, 100, 999, 549.5)),\n",
              " (4, (9000, 1000, 9999, 5499.5)),\n",
              " (5, (90000, 10000, 99999, 54999.5)),\n",
              " (6, (900000, 100000, 999999, 549999.5))]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter**: Identify how many *five-digit* numbers are divisible by *7*."
      ],
      "metadata": {
        "id": "c6PK09Pv-Waw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_digit_num_rdd.filter(lambda x: x[0] == 5 and x[1] % 7 == 0).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-enMTzjd-04K",
        "outputId": "b9f9b4cf-331b-4711-837f-0ea39675eebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12857"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mapPartitions**: Count the number of elements in each partition (slice)"
      ],
      "metadata": {
        "id": "6OyXQJVFciOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(iterator): yield sum(1 for _ in iterator)\n",
        "count_per_partition = range_rdd.mapPartitions(f).collect()\n",
        "print(f'Number of points per partition = {count_per_partition}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkMwepjEyO_k",
        "outputId": "012e05ac-ef4c-4f9f-e6f8-fe83fcaeae39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of points per partition = [250000, 250000, 250000, 250000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List of RDD Transformations\n",
        "\n",
        "- **map(func)**\tReturn a new distributed dataset formed by passing each element of the source through a function func.\n",
        "\n",
        "- **filter(func)**\tReturn a new dataset formed by selecting those elements of the source on which func returns true.\n",
        "\n",
        "- **distinct**([numPartitions]))\tReturn a new dataset that contains the distinct elements of the source dataset.\n",
        "\n",
        "- **flatMap(func)**\tSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n",
        "\n",
        "- **sample**(withReplacement, fraction, seed)\tSample a fraction of the data, with or without replacement, using a given random number generator seed.\n",
        "\n",
        "- **union**(otherDataset)\tReturn a new dataset that contains the union of the elements in the source dataset and the argument.\n",
        "\n",
        "- **intersection**(otherDataset)\tReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
        "\n",
        "- **groupByKey**([numPartitions])\tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
        "Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
        "Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks.\n",
        "\n",
        "- **reduceByKey**(func, [numPartitions])\tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
        "\n",
        "- **aggregateByKey**(zeroValue)(seqOp, combOp, [numPartitions])\tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
        "\n",
        "- **sortByKey**([ascending], [numPartitions])\tWhen called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.\n",
        "\n",
        "- **join**(otherDataset, [numPartitions])\tWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.\n",
        "cogroup(otherDataset, [numPartitions])\tWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.\n",
        "\n",
        "- **mapPartitions(func)**\tSimilar to map, but runs separately on each partition (block) of the RDD, so func must be of type `Iterator<T> => Iterator<U>` when running on an RDD of type T.\n",
        "\n",
        "- **mapPartitionsWithIndex(func)**\tSimilar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type `(Int, Iterator<T>) => Iterator<U>` when running on an RDD of type T.\n",
        "\n",
        "- **cartesian**(otherDataset)\tWhen called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).\n",
        "\n",
        "- **pipe**(command, [envVars])\tPipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.\n",
        "\n",
        "- **coalesce**(numPartitions)\tDecrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.\n",
        "\n",
        "- **repartition**(numPartitions)\tReshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.\n",
        "\n",
        "- **repartitionAndSortWithinPartitions**(partitioner)\tRepartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery."
      ],
      "metadata": {
        "id": "ceAIM_6rOksq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List of RDD Actions\n",
        "\n",
        "- **count()**\tReturn the number of elements in the dataset.\n",
        "\n",
        "- **countByKey()**\tOnly available on RDDs of type (K, V).\n",
        "Returns a hashmap of (K, Int) pairs with the count of each key.\n",
        "\n",
        "- **collect()**\tReturn all the elements of the dataset as an array at the driver program.\n",
        "\n",
        "- **first()**\tReturn the first element of the dataset (similar to take(1)).\n",
        "\n",
        "- **take(n)**\tReturn an array with the first n elements of the dataset.\n",
        "\n",
        "- **takeOrdered**(n, [ordering])\tReturn the first n elements of the RDD using either their natural order or a custom comparator.\n",
        "\n",
        "- **takeSample**(withReplacement, num, [seed])\tReturn an array with a random sample of num elements of the dataset, with or without replacement. âš  Do not load the whole RDD to driver memory. Consider using `sample` transformation instead.\n",
        "\n",
        "- **reduce**(func)\tAggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be *commutative* and *associative* so that it can be computed correctly in parallel.\n",
        "\n",
        "- **saveAsTextFile**(path)\tWrite the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\n",
        "\n",
        "- **foreach(func)**\tRun a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.\n"
      ],
      "metadata": {
        "id": "OlflvrheOODC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Tips\n",
        "In this section, we will discuss some performance issues and best practices in Spark.\n",
        "\n",
        "Let's begin with how Spark saves all data in memory. As you may have heard, Spark stores data in memory to avoid expensive disk I/O operations. This is the key reason Spark outperforms Hadoop MapReduce and achieves its lightning speed.\n",
        "\n",
        "Now, consider this scenario: Imagine we have a huge dataset, say 10 billion numbers, saved in an RDD. As we know, Spark distributes this data into smaller partitions across the nodes in the cluster. Right? Let's say we perform some *transformations* followed by *actions* on the original data, creating a new RDD. This new RDD may also fit into memory. However, if we continue in this manner, we will eventually reach a point where the new RDDs no longer fit in memory. What happens then?\n",
        "\n",
        "The answer lies in how Spark handles data. First, we should remember that a new RDD is only generated when an action is called. Otherwise, no new RDD is created. The Directed Acyclic Graph (DAG) keeps track of the transformations applied to the original RDD to create a new RDD. Every time a new RDD is created, Spark tries to save it in memory by default. However, if the new RDD doesnâ€™t fit in memory, Spark does not **cache** it. Instead, it will regenerate the RDD on the fly each time it is needed.\n",
        "\n",
        "Generally, Spark uses three storage levels:\n",
        "\n",
        "- **MEMORY_ONLY**: If the RDD does not fit in memory, some partitions will not be cached and will be *recomputed* on the fly each time they are needed. This is the default storage level.\n",
        "\n",
        "- **MEMORY_AND_DISK**: If the RDD does not fit in memory, the partitions that don't fit will be *stored on disk* and read from there when needed.\n",
        "\n",
        "- **DISK_ONLY**: The RDD partitions are stored only on disk.\n",
        "\n",
        "**Catch**\n",
        "\n",
        "Suppose you have completed a sequence of time-consuming transformations, and you want to prevent Spark from repeating these operations each time. In this case, you may want to save the results in memory for future use. Two methods come in handy: `cache()` and `persist()`.\n",
        "\n",
        "**unpersist**\n",
        "\n",
        "On the other hand, if you want to identify RDDs that are no longer important, and you want Spark to consider removing these from memory when necessary, `unpersist()` is your friend.\n"
      ],
      "metadata": {
        "id": "siHl_iRAgeQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "def is_prime(n):\n",
        "  if n < 2:\n",
        "    return False\n",
        "  for i in range(2, int(n**0.5) + 1):\n",
        "    if n % i == 0:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "# Create an RDD\n",
        "range_rdd = sc.parallelize(range(1, 1_000_000), 4)\n",
        "\n",
        "# Perform a task without caching\n",
        "prime_numbers = range_rdd.filter(is_prime)\n",
        "t1 = perf_counter()\n",
        "result_no_cache = (prime_numbers.count(),  prime_numbers.sum())\n",
        "t2 = perf_counter()\n",
        "print(f\"Result without caching: {result_no_cache}\")\n",
        "\n",
        "# Perform a task with caching\n",
        "prime_numbers2 = range_rdd.filter(is_prime).cache()\n",
        "t3 = perf_counter()\n",
        "result_cache = (prime_numbers2.count(), prime_numbers2.sum())\n",
        "t4 = perf_counter()\n",
        "print(f\"Result with caching: {result_cache}\")\n",
        "\n",
        "print()\n",
        "print(f\"Time without caching: {t2 - t1:.4f} seconds\")\n",
        "print(f\"Time with caching: {t4 - t3:.4f} seconds\")\n",
        "print()\n",
        "print('storage level(without caching): ', prime_numbers.getStorageLevel())\n",
        "print('storage level(with caching): ', prime_numbers2.getStorageLevel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2lcx3xfrY4U",
        "outputId": "77d2f7f5-1be1-4b72-aa82-51b23ae2ceae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result without caching: (78498, 37550402023)\n",
            "Result with caching: (78498, 37550402023)\n",
            "\n",
            "Time without caching: 13.9924 seconds\n",
            "Time with caching: 7.1529 seconds\n",
            "\n",
            "storage level(without caching):  Serialized 1x Replicated\n",
            "storage level(with caching):  Memory Serialized 1x Replicated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unpersisting Data\n",
        "Spark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the RDD.`unpersist()` method. Note that this method does not block by default. To block until resources are freed, specify blocking=true when calling this method."
      ],
      "metadata": {
        "id": "5vSuMCQAY0-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_rdd.cache()\n",
        "print('storage level(cache): ', range_rdd.getStorageLevel())\n",
        "range_rdd.unpersist(blocking=True)\n",
        "print('storage level(unpersist): ', range_rdd.getStorageLevel())"
      ],
      "metadata": {
        "id": "0ZU3IbwQY-SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_rdd.take(5)"
      ],
      "metadata": {
        "id": "rEO4PE8OazSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accumulators\n",
        "ðŸ’¡ Supporting general, read-write **shared** variables across tasks would be inefficient.\n",
        "\n",
        " However, Spark does provide two limited types of shared variables for two common usage patterns:\n",
        "\n",
        " - broadcast variables\n",
        " - accumulators.\n",
        "\n",
        "Accumulators in Spark are used specifically to provide a mechanism for safely updating a **shared** variable when execution is split up across worker nodes in a cluster."
      ],
      "metadata": {
        "id": "Tw0PYLjZCj5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accum = sc.accumulator(0)\n",
        "\n",
        "def count_even(x):\n",
        "  global accum\n",
        "  if x%2 == 0:\n",
        "    accum +=1\n",
        "\n",
        "sc.parallelize(range(0,100_000)).foreach(count_even)\n",
        "print(f'accumulated value = {accum.value:,}')\n"
      ],
      "metadata": {
        "id": "08rgKI6lxqlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples"
      ],
      "metadata": {
        "id": "WgF1txhqDezS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Word Counter\n",
        "**Task 1**: Count the occurrences of each word in a text document and sort the results based on frequency.\n"
      ],
      "metadata": {
        "id": "JIr4-YGPs0xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd=sc.textFile('/data/text.txt')\n",
        "count_text_rdd=text_rdd.flatMap(lambda x: x.lower().split(' '))\\\n",
        "        .map(lambda x: (x, 1))\\\n",
        "        .reduceByKey(lambda x, y: x+y)\\\n",
        "        .sortBy(lambda x: x[1], ascending=False)\n",
        "count_text_rdd.take(10)"
      ],
      "metadata": {
        "id": "1UI-aMaNs-Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**: Group words based on the number of characters in each word, and identify the number of *unique* words **in** each group.\n"
      ],
      "metadata": {
        "id": "jeCPyM1jykye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_text_rdd.map(lambda x: (len(x[0]), 1))\\\n",
        "        .reduceByKey(lambda x, y: x+y)\\\n",
        "        .sortBy(lambda x: x[0] , ascending=False)\\\n",
        "        .take(5)"
      ],
      "metadata": {
        "id": "5gbA1h5qyu0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Computing $\\pi$\n"
      ],
      "metadata": {
        "id": "ZoeKC43rSp2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def sample(_):\n",
        "  x, y = random.random(), random.random()\n",
        "  return 1 if x*x + y*y <= 1 else 0\n",
        "\n",
        "N= 1_000_000\n",
        "count = sc.parallelize(range(0, N)).map(sample)\\\n",
        "             .sum()\n",
        "print(f\"Pi is roughly {4.0 * count / N}\")"
      ],
      "metadata": {
        "id": "Z0IbxUn5bkyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Spark\n",
        "At the end, you should close the connetion to Spark by calling `stop` method."
      ],
      "metadata": {
        "id": "Cv6IjL--ePYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "sb1S7PvweLMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations) on the Spark website.\n",
        "2. [Apache Spark / PySpark Tutorial: Basics In 15 Minutes](https://www.youtube.com/watch?v=QLQsW8VbTN4) - A concise and informative YouTube video by Greg Hogg that describes the basics of working with RDD in PySpark.\n",
        "3. [PySpark Tutorial: Spark SQL & DataFrame Basics](https://www.youtube.com/watch?v=3-pnWVWyH-s) - Another great video by Greg Hogg, focusing on using PySpark DataFrames.\n",
        "4. [Introduction to Google Colab and PySpark](https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/#big-data-pyspark-and-colaboratory) - Learn how to set up Google Colab for using PySpark, along with a thorough review of PySpark's capabilities.\n",
        "5. [Spark with Python](https://github.com/tirthajyoti/Spark-with-Python/tree/master) - A GitHub repository that includes several Jupyter notebooks with examples of how to use PySpark.\n",
        "6. [Apache Sparkâ„¢ examples](https://spark.apache.org/examples.html)\n",
        "7. [Apache Spark GitHub python examples](https://github.com/apache/spark/tree/master/examples/src/main/python)\n"
      ],
      "metadata": {
        "id": "X0TmGVBb5dMa"
      }
    }
  ]
}